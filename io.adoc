:github: https://github.com/dsyer/reactive-notes
:master: {github}/blob/master
:partii: {master}/flux.adoc
:partiv: {master}/platform.adoc

In this article we continue the series on {partii}[Reactive Programming], and the focus is less on learning the basic APIs and more on more concrete use cases and writing code that actually does something useful. We will see how Reactive is a useful abstraction for concurrent programming, but also that it has some very low level features that we should learn to treat with respect and caution. If we start to use these features to their full potential we can take control of layers in our application that previously were invisible, hidden by containers, platforms and frameworks.

== Bridging from Blocking to Reactive with Spring MVC

Being Reactive forces you to look at the world differently. Instead of asking for something and getting it (or not getting it), everything is delivered as a sequence (`Publisher`) and you have to subscribe to it. Instead of waiting for an answer, you have to register a callback. It's not so hard when you get used to it, but unless the whole world turns on its head and becomes Reactive, you are going to find you need to interact with an old-style blocking API

Suppose we have a back end service, and we want to call it repeatedly with different arguments and aggregate the results. It's a classic "scatter-gather" use case, which you would get, for instance, if you had a paginated back end needed to summarize the "top N" items across multiple pages. Since the details of the non-reactive (blocking) operation are not relevant to the scatter-gather pattern, we can push them down into a method called `block()`, and implement it later. Here's a (bad) example that calls the back end and aggregates into an object of type `Result`:

```java
Flux.range(1, 10) // <1>
    .log()
    .map(value -> block(value)) // <2>
    .collect(Result::new, Result::add) // <3>
    .doOnSuccess(Result::stop) // <4>
```
<1> make 10 calls
<3> blocking code here
<3> collect results and aggregate into a single object
<4> at the end stop the clock (the result is a `Mono<Result>`)

Don't do this at home. It's a "bad" example because, while the APIs are technically being used correctly, we know that it is going to block the calling thread; this code is more or less equivalent to a for loop with the call to `block()` in the body of the loop. A better implementation would push the `block()` onto a background thread. We can do that with a `flatMap()` instead of a `map()`:

```java
Flux.range(1, 10)
    .log()
    .flatMap( // <1>
        value -> Mono.fromCallable(() -> block(value)) // <2>
                    .subscribeOn(this.scheduler), // <3>
    4) // <4>
    .collect(Result::new, Result::add)
    .doOnSuccess(Result::stop)
```
<1> drop down to a new publisher to process in parallel
<2> blocking code here inside a Callable to defer execution
<3> subscribe to the slow publisher on a background thread
<4> concurrency hint in flatMap

The `scheduler` is declared separately as a shared field: `Scheduler scheduler = Computations.parallel("sub")`.

=== Embedding in a Non-Reactive Server

If we wanted to run the scatter-gather code in a non-reactive server like a servlet container, we could use Spring MVC, like this:


```java
@RequestMapping("/parallel")
public CompletableFuture<Result> parallel() {
    return Flux.range(1, 10)
      ...
      .doOnSuccess(Result::stop)
      .toCompletableFuture();
}
```

If you read the Javadocs for `@RequestMapping` you will find that a method can return a `CompletableFuture` "which the application uses to produce a return value in a separate thread of its own choosing". The separate thread in this case is provided by "scheduler", which is a thread pool, so the processing is happening on multiple threads, 4 at a time, because of the way that `flatMap()` is called.

=== No Such Thing as a Free Lunch

The scatter-gather with a background thread is a useful pattern but it isn't perfect -- it's not blocking the caller, but it's blocking something, so it's just moving the problem around. There are some practical implications. We have an HTTP server with (probably) non-blocking IO handlers, passing work back to a thread pool, one HTTP request per thread -- all of this is happening inside a servlet container (e.g. Tomcat). The request is processed asynchronously, so the worker thread in Tomcat isn't blocked, and the thread pool that we created in our "scheduler" is processing on up to 4 concurrent threads. We are processing 10 back end requests (calls to `block()`) so there is a maximum, theoretical benefit of using the scheduler of 10/4=2.5 times lower latency. In other words, if processing all 10 requests one after the other in a single thread takes 1000ms, we might see a processing time of 400ms for a single incoming request at our HTTP service. We should emphasise the "might" though: it's only going to go that fast if there is no contention for the processing threads (in both stages, the Tomcat workers, and the application scheduler). If you have very low concurrency, i.e. a small number of clients connecting to your application, and hardly any chance that two will make a request at the same time, then you will probably see close to the theoretical improvement. As soon as there are multiple clients trying to connect, they will all be competing for the same 4 threads, and the latency will drift up, and could even be worse than that experienced by a single client with no background processing. We can improve the latency for concurrent clients by creating the scheduler with a larger thread pool, e.g.

```java
    private Scheduler scheduler = Computations.parallel("sub", 64, 16);
```

(16 threads and a buffer of 64 items waiting to be processed.) Now we are using more memory for the threads and their stacks, and we can expect to see lower latency for low concurrency, but not necessarily for high concurrency if our hardware has fewer than 16 cores. We also do not expect higher throughput under load: if there is contention for the threads, there is a high cost for managing those resources and that has to be reflected somwehere in a metric that matters. If you are interested in more detailed analysis of that kind of trade off, some detailed analyses of how performance metrics scale under load can be found in a blog series by https://robharrop.github.io/[Rob Harrop].

== Reactive all the Way Down

The bridge from blocking to reactive is a useful pattern, and is easy to implement using available technology in Spring MVC (as shown above). The next stage in the Reactive journey is to break out of blocking in application threads completely, and to do that requires new APIs and new tools. Ultimately we have to be Reactive all the way down the stack, including servers and clients. This is the goal of Spring Reactive, which is a new framework, orthogonal to Spring MVC, but meeting the same needs, and using a similar programming model.

NOTE: Spring Reactive started as a standalone project, but is be folded into the Spring Framework in version 5.0 (first milestone June 2016).

The first step to fully Reactive in our scatter-gather example would be to replace `spring-boot-starter-web` with `spring-boot-starter-reactive-web` on the classpath. In Maven:

```xml
<dependency>
	<!-- github: bclozel/spring-boot-reactive-web -->
	<groupId>org.springframework.boot</groupId>
	<artifactId>spring-boot-starter-reactive-web</artifactId>
	<version>0.0.1-SNAPSHOT</version>
</dependency>
```

or in Gradle:

```groovy
repositories {
    ...
    mavenLocal()
}


dependencies {
	compile('org.springframework.boot:spring-boot-starter-reactive-web:0.0.1-SNAPSHOT')
    ...
}
```

(At this point you need to build and install this starter locally from https://github.com/bclozel/spring-boot-reactive-web[GitHub].)

Then in the controller, we can simply lose the bridge to `CompletableFuture` and return an object of type `Mono`:

```java
@RequestMapping("/parallel")
public Mono<Result> parallel() {
    return Flux.range(1, 10)
            .log()
            .flatMap(
                    value -> Mono.fromCallable(() -> block(value))
                            .subscribeOn(scheduler),
                    4)
            .collect(Result::new, Result::add)
            .doOnSuccess(Result::stop);
}
```

Take this code and put it in a Spring Boot application and it will run in Tomcat, Jetty or Netty, depending on what it finds on the classpath. Tomcat is the default server in that starter, so you have to exclude it and provide a different one if you want to switch. All three have very similar characteristics in terms of startup time, memory usage and runtime resource usage.

We still have the blocking backend call in `block()`, so we still have to subscribe on a thread pool to avoid blocking the caller. We can change that if we have a non-blocking client, e.g. instead of using `RestTemplate` we use the new `WebClient`. For example, if the `block()` method was implemented like this:

```java
private RestTemplate restTemplate = new RestTemplate();
tory());

private HttpStatus block(int value) {
    return this.restTemplate.getForEntity("http://example.com", String.class, value)
            .getStatusCode();
}
```

then we might do this instead to use a non-blocking client:

```java
private WebClient client = new WebClient(new ReactorHttpClientRequestFactory());

private Mono<HttpStatus> fetch(int value) {
    return this.client.perform(HttpRequestBuilders.get("http://example.com"))
            .extract(WebResponseExtractors.response(String.class))
            .map(response -> response.getStatusCode());
}
```

Note that the `WebClient.perform()` has a Reactive return type, which we have transformed into a `Mono<HttpStatus>`, but we have not subscribed to it. We want the framework to do all the subscribing, so now we are Reactive all the way down.

Now we can swap the call to `block()` for a call to `fetch()` in the HTTP request handler:

```java
@RequestMapping("/netty")
public Mono<Result> netty() {
    return Flux.range(1, 10) // <1>
        .log() //
        .flatMap(this::fetch) // <2>
        .collect(Result::new, Result::add)
        .doOnSuccess(Result::stop);
}
```
<1> make 10 calls
<2> drop down to a new publisher to process in parallel

This code is a lot cleaner than when we had to bridge to the blocking client, which can be attributed to the fact that the code is Reactive all the way down. The Reactive `WebClient` returns a `Mono`, and that drives us immediately to select `flatMap()` in the transformation chain, and the code we need just falls out. It's a nicer experience to write it, and it's more readable, so it's easier to maintain.

WARNING: Methods in Spring Reactive that return a `Publisher` *are* non-blocking, but in general a method that returns a `Publisher` (or `Flux`, `Mono` or `Observable`) is only a hint that it might be non-blocking. If you are writing such methods it is important to analyse (and preferably test) whether they block, and to let callers know explicitly if they might do.

Remember the same application code runs on Tomcat, Jetty or Netty. Currently, the Tomcat and Jetty support is provided on top of Servlet 3.1 asynchronous processing, so it is limited to one request per thread. When the same code runs on the Netty server platform that constraint is lifted, and the server can dispatch requests sympathetically to the web client. As long as the client doesn't block, everyone is happy. Performance metrics for the netty server and client probably show similar characteristics, but the Netty server is not restricted to processing a single request per thread, so it doesn't use a large thread pool and we might expect to see some differences in resource utilization. We will come back to that later in another article in this series.

TIP: in the {github}[sample code] the "reactive" sample has Maven profiles "tomcat", "tomcatNext" (for Tomcat 8.5, has to be enabled *with* "tomcat"), "jetty" and "netty", so you can easily try out all the different server options without changing a line of code.

NOTE: the blocking code in many applications is not HTTP backend calls, but database interactions. Very few databases support non-blocking clients at this point in time (MongoDB and Counchbase are notable exceptions, but even those are not as mature as the HTTP clients). Thread pools and the blocking-to-reactive pattern will have a long life until all the database vendors catch up on the client side.

== Conclusion

It's nice to be able to control all the moving parts in our asynchronous processing: every layer has a thread pool size and a queue. But at some point it becomes a burden, and we start looking for something simpler, in some sense. Sober analysis of scalability leads to the conclusion that it is often better to shed those extra threads, and work with the constraints imposed by the physical hardware. This is an example of "mechanical sympathy", as is famously exploited by LMAX to great effect in the https://lmax-exchange.github.io/disruptor/[Disruptor Pattern].

We have begun to see the power of the Reactive approach, but remember that with power comes responsibility. It's radical, and it's fundamental. It's "rip it up and start again" territory. So you will also hopefully appreciate that Reactive isn't a solution to all problems. In fact it isn't a solution to any problem, it merely facilitates the solution of a certain class of problems. The benefits you get from using it might be outweighed by the costs of learning it, modifying your APIs to be Reactive all the way down, and maintaining the code afterwards, so tread carefully.



